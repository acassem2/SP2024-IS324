{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer() #Initialize lemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We are going to use email data from program-l mailing list to build edges and nodes for the graph.\n",
    ">* https://www.freelists.org/archive/program-l\n",
    "> * Let's read file using `pd.read_csv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('thread_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* In `data` dataframe, we have six columns, each representing as follows:\n",
    "    \n",
    "    * `thread_id` : unique id for each thread\n",
    "    * `thread_name` : the first subject of the email\n",
    "    * `body` : the content of the email \n",
    "    * `account` : the email account of the sender \n",
    "    * `url` : the url of the email\n",
    "    * `date` : the date of the email "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Regular Expression (RegEx) (2pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. RegEx to extract usernames (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['chojiro1990' 'soronel.haetir' 'chojiro1990' 'soronel.haetir'\\r\\n 'chojiro1990' 'soronel.haetir' 'chojiro1990' 'soronel.haetir'\\r\\n 'chojiro1990']\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['account'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [chojiro1990, soronel.haetir, chojiro1990, sor...\n",
       "1      [hedvig.jung, dzhovani.chemishanov, hedvig.jung]\n",
       "2     [justind, david.lant, justind, travis, justind...\n",
       "3     [birkir.gunnarsson, lras, birkir.gunnarsson, j...\n",
       "4     [david.lant, markalong64, jamal.mazrui, david....\n",
       "                            ...                        \n",
       "95                             [cmusic789, james.homme]\n",
       "96    [m10fayed, abletec, ntsiklauri2, spg1111, m10f...\n",
       "97                     [jhomme, jhomme, isidor.nikolic]\n",
       "98    [programmer651, james.corbett, joseph.lee22590...\n",
       "99    [pmorales, rbreiten, jude.dashiell, jude.dashi...\n",
       "Name: account_list, Length: 100, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r\"'([^']+)'\")\n",
    "data['account_list'] = data['account'].apply(lambda x: re.findall(pattern, x))\n",
    "data['account_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. RegEx to extract date (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['2013-06-05T09:50:54.000000000' '2013-06-05T15:42:35.000000000'\\r\\n '2013-06-05T16:28:36.000000000' '2013-06-05T19:52:01.000000000'\\r\\n '2013-06-06T09:33:53.000000000' '2013-06-06T15:31:31.000000000'\\r\\n '2013-06-06T19:15:11.000000000' '2013-06-06T20:04:44.000000000'\\r\\n '2013-06-07T05:05:21.000000000']\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['date'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2013-06-05T09:50:54.000000000',\n",
       " '2013-06-05T15:42:35.000000000',\n",
       " '2013-06-05T16:28:36.000000000',\n",
       " '2013-06-05T19:52:01.000000000',\n",
       " '2013-06-06T09:33:53.000000000',\n",
       " '2013-06-06T15:31:31.000000000',\n",
       " '2013-06-06T19:15:11.000000000',\n",
       " '2013-06-06T20:04:44.000000000',\n",
       " '2013-06-07T05:05:21.000000000']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{9}\")\n",
    "data['date_list'] = data['date'].apply(lambda x: re.findall(pattern, x))\n",
    "data['date_list'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pandas and Text mining (6pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. Count the number of users involved in the email conversation (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      9\n",
       "1      3\n",
       "2      7\n",
       "3      4\n",
       "4      8\n",
       "      ..\n",
       "95     2\n",
       "96     5\n",
       "97     3\n",
       "98     5\n",
       "99    14\n",
       "Name: account_list, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "data[\"account_list\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26    38\n",
       "Name: account_list, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"account_list\"].apply(lambda x: len(x)).nlargest(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3170"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[26].thread_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. Lowercase the body column (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [\"hi people! right now, i'm currently doing a ...\n",
       "1     ['i am. i write you from budapest, hungary. i ...\n",
       "2     [\"hi all. i'm possibly needing to use windows ...\n",
       "3     ['gang i\\'m trying to use vc # express 2008. o...\n",
       "4     [\"hi all, i have a vb form which shows a numbe...\n",
       "                            ...                        \n",
       "95    ['hi all. i ’ m looking for an accessible ( ke...\n",
       "96    ['hi, my mail find you well, i ’ m a iphone us...\n",
       "97    [\"hi, user settings json file has errors it i ...\n",
       "98    ['tell me if you guys get an attachment if i s...\n",
       "99    ['hi all. as i said days ago. i worked program...\n",
       "Name: body_lower, Length: 100, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "data['body_lower']=data['body'].str.lower()\n",
    "data['body_lower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3. Removing stopwords (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [\"hi people! right now, i'm currently project ...\n",
       "1     ['i am. write budapest, hungary. work area vis...\n",
       "2     [\"hi all. i'm possibly needing use windows xp ...\n",
       "3     ['gang i\\'m trying use vc # express 2008. firs...\n",
       "4     [\"hi all, vb form shows number rows controls. ...\n",
       "                            ...                        \n",
       "95    ['hi all. ’ looking accessible ( keyboard + sc...\n",
       "96    ['hi, mail find well, ’ iphone user. ’ contact...\n",
       "97    [\"hi, user settings json file errors can't get...\n",
       "98    ['tell guys get attachment send one. recently,...\n",
       "99    ['hi all. said days ago. worked programming da...\n",
       "Name: stopword, Length: 100, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "stop=stopwords.words('english')\n",
    "data['stopword']=data['body_lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "data['stopword']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-4. Tokenization and removing punctuations (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [hi, people, right, now, i, currently, project...\n",
       "1     [i, am, write, budapest, hungary, work, area, ...\n",
       "2     [hi, all, i, possibly, needing, use, windows, ...\n",
       "3     [trying, use, vc, express, first, page, open, ...\n",
       "4     [hi, all, vb, form, shows, number, rows, contr...\n",
       "                            ...                        \n",
       "95    [all, looking, accessible, keyboard, screen, r...\n",
       "96    [mail, find, well, iphone, user, contacted, pa...\n",
       "97    [hi, user, settings, json, file, errors, ca, g...\n",
       "98    [guys, get, attachment, send, one, recently, c...\n",
       "99    [all, said, days, ago, worked, programming, da...\n",
       "Name: punct_token, Length: 100, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "data['token']=data['stopword'].apply(word_tokenize)\n",
    "data['punct_token']=data['token'].apply(lambda x: [word for word in x if word.isalnum()])\n",
    "data['punct_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-5. Lemmatization (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [hi, people, right, now, i, currently, project...\n",
       "1     [i, am, write, budapest, hungary, work, area, ...\n",
       "2     [hi, all, i, possibly, needing, use, windows, ...\n",
       "3     [trying, use, vc, express, first, page, open, ...\n",
       "4     [hi, all, vb, form, shows, number, rows, contr...\n",
       "                            ...                        \n",
       "95    [all, looking, accessible, keyboard, screen, r...\n",
       "96    [mail, find, well, iphone, user, contacted, pa...\n",
       "97    [hi, user, settings, json, file, errors, ca, g...\n",
       "98    [guys, get, attachment, send, one, recently, c...\n",
       "99    [all, said, days, ago, worked, programming, da...\n",
       "Name: punct_token, Length: 100, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer() #Initialize lemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "data[\"punct_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'): #ADJECTIVE\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'): #VERN\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'): #NOUN        \n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'): #ADVERB\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    # Tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged) \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # If no tag was found, then use the word as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            # Else use the tag to lemmatize the word\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [hi, people, right, now, i, currently, project...\n",
       "1     [i, be, write, budapest, hungary, work, area, ...\n",
       "2     [hi, all, i, possibly, need, use, window, xp, ...\n",
       "3     [try, use, vc, express, first, page, open, app...\n",
       "4     [hi, all, vb, form, show, number, row, control...\n",
       "                            ...                        \n",
       "95    [all, look, accessible, keyboard, screen, read...\n",
       "96    [mail, find, well, iphone, user, contact, pack...\n",
       "97    [hi, user, setting, json, file, error, ca, get...\n",
       "98    [guy, get, attachment, send, one, recently, cr...\n",
       "99    [all, say, day, ago, work, program, database, ...\n",
       "Name: lemma, Length: 100, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemma'] = data['punct_token'].apply(lambda x: lemmatize_sentence(' '.join(x))).apply(lambda x: word_tokenize(x))\n",
    "data['lemma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-6. Count the frequency of unigrams (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('use', 440),\n",
       " ('would', 275),\n",
       " ('work', 250),\n",
       " ('get', 234),\n",
       " ('i', 233),\n",
       " ('file', 192),\n",
       " ('know', 177),\n",
       " ('like', 177),\n",
       " ('line', 174),\n",
       " ('code', 165)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "Counter([item for row in data['lemma'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building edges (2pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1. Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2820\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "import itertools\n",
    "edges=[]\n",
    "for idx, val in data['account_list'].items():\n",
    "        edges.extend(list(itertools.combinations(val, 2)))\n",
    "\n",
    "print(len(edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2. Removing self-loop (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges before removing self-loops: 2820\n",
      "Number of edges after removing self-loops: 2335\n"
     ]
    }
   ],
   "source": [
    "edges_loop = [edge for edge in edges if edge[0] != edge[1]]\n",
    "\n",
    "print(\"Number of edges before removing self-loops:\", len(edges))\n",
    "print(\"Number of edges after removing self-loops:\", len(edges_loop))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

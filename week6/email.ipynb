{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We are going to use email data from program-l mailing list to build edges and nodes for the graph.\n",
    ">* https://www.freelists.org/archive/program-l\n",
    "> * Let's read file using `pd.read_feather` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* feather file is a fast, lightweight, and easy-to-use binary file format for storing data frames. \n",
    ">* Downloading pre-requisite libraries may necessary to read feather file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_feather('sample-feather.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's check the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's check datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's check the first few rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* In `data` dataframe, we have six columns, each representing as follows:\n",
    "    \n",
    "    * `thread_id` : unique id for each thread\n",
    "    * `thread_name` : the first subject of the email\n",
    "    * `body` : the content of the email \n",
    "    * `account` : the email account of the sender \n",
    "    * `url` : the url of the email\n",
    "    * `date` : the date of the email "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Think of thread as an email conversation. `thread_id` is the unique id for the email conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's check which thread has the most number of accounts involved in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['account'].apply(lambda x: len(x)).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The index of 39 has 43 users involved in the conversation.\n",
    "> * Let's see who are the users involved in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[39, 'account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We can see some of the users are repeating, meaning they are involved in the conversation multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We want to see the unique users involved in the conversation.\n",
    ">* To do so, we want to use `nunique()` function to get the number of unique elements in `pd.Series` object.\n",
    "> * So, we have to convert the list into `pd.Series` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['account'].apply(lambda x: pd.Series(x).nunique()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We can still find the index 39 has the most unique users involved in the conversation.\n",
    "> * But the third most unique users involved in the conversation is different from the most users involved in the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's do text mining on the `body` column to find the most common words used in the conversation.\n",
    "> * To do so, let's import necessary libraries we practiced in the previous classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We learned the importance of pre-processing before doing text mining.\n",
    "> * Lowercasing, removing punctuation, removing stop words, and tokenization are the most common pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* The body column is a list. We have to join the strings in the list to make it a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body-str'] = data['body'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's lowercase the body column first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body-lower']=data['body-str'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['body-str'].iloc[0]) #before lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['body-lower'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Okay! lowercasing is done. Now, let's remove the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')\n",
    "#loading stopwords in the variable named stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stopword']=data['body-lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "#The lambda function takes each row of the 'body-lower' column, splits it into a list of words, \n",
    "#and then joins the words back together into a string, excluding any words that are in the 'stop' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body-lower'].iloc[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stopword'].iloc[98]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * This time, let's do tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['token']=data['stopword'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['token'].iloc[98][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Finally let's get rid of the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['punct_token']=data['token'].apply(lambda x: [word for word in x if word.isalnum()])\n",
    "#if the string is alphanumeric, it is included in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['punct_token'].iloc[98][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We are interested in finding the most common words used in the thread (email conversation) index 39."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(data['punct_token'].iloc[98]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's see which thread has been alive for the longest time.\n",
    "> * Some email lasts for a few days, some for a few months, and some for a few years.\n",
    "> * We can calculate the time difference between the first and the last email of the thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'].apply(lambda x: x.max())\n",
    "#max() function will return the latest date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'].apply(lambda x: x.min())\n",
    "#min() function will return the earliest date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'].apply(lambda x: x.max()-x.min()).sort_values(ascending=False) \n",
    "#combining max() and min() function together will reutrn the difference between the latest and earliest date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Okay, the thread with index 6 has been conversing more than 1000 days!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's see what users have been talking about in the thread with index 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[6, 'body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[6, 'date']\n",
    "#The earlist date of this conversation is 2015-12-22 and the latest date is 2020-09-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's see the most common words used in the thread that has been alive for the longest time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(data['punct_token'].iloc[6]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's jump into the network part of this data.\n",
    ">* Always remember there are three main components of a network: nodes, edges, and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* How do you want to design the graph with the given data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.path_graph(5)\n",
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=nx.complete_graph(5)\n",
    "nx.draw(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * If we think about directionality, the path graph will look like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.path_graph(5, create_using=nx.DiGraph())\n",
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* But given the nature of back-and-forth conversation in the email, there is high likelihood that the graph will be undirected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_directed=nx.complete_graph(5, create_using=nx.DiGraph())\n",
    "nx.draw(C_directed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's think about nodes\n",
    "> * Where can we get the nodes from? It is in the `account` column but data is in the list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* How many unique nodes are there in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([item for sublist in data['account'] for item in sublist]).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's build edges between users in the conversation (thread).\n",
    "> * To do so, we will use the `account` column and iterate over the rows to created edges between users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will need a combination of all the accounts in the 'account' column to create the edges of the graph\n",
    "#We will use itertools.combinations to create the combination\n",
    "\n",
    "import itertools\n",
    "edges=[]\n",
    "for idx, val in data['account'].items():\n",
    "    edges.extend(list(itertools.combinations(val, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's get rid of the self-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_loop = [edge for edge in edges if edge[0] != edge[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_loop[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We can also get rid of the duplicate edges if we want to design the graph as an unwieghted graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_loop = list(set(edges_loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_loop[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's see who has the highest degree centrality in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree={}\n",
    "for element in pd.Series([item for sublist in data['account'] for item in sublist]).unique():\n",
    "    count=0\n",
    "    for edge in edges_loop:\n",
    "        if element in edge:\n",
    "            count+=1\n",
    "    degree[element]=count    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * To sort degree based on the value of the degree, we can use `sorted` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x = sorted(degree.items(), key=lambda k: k[1], reverse=True)\n",
    "sorted_dict = dict(sorted_x)\n",
    "dict(list(sorted_dict.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality={}\n",
    "for key, value in sorted_dict.items():\n",
    "    degree_centrality[key]=value/(len(pd.Series([item for sublist in data['account'] for item in sublist]).unique())-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* `jacobk` has the highest degree centrality in the graph.\n",
    ">* Let's compare `jacobk` degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality['jacobk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* This time, we want to subset the data to only include the conversation that has involved `jacobk`.\n",
    ">* Hint! `isin` function can be useful. Remember `isin()` function is from `pd.Series` object.\n",
    ">* Also, try `.apply()` and `lambda` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's put the result in `jacobkdf` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We are curious about the most common words that `jacobk` has used in the conversation.\n",
    ">* We have to use `body` column because in `punct-token` we already joined all the strings in the `body` column.\n",
    ">* The strings in `body` column follow the order in the `account` column, meaning the first string in `body` column has been sent by the first user in the `account` column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's find the index (order) of `jacobk` in the `account` column in the `jacobkdf` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's print what `jacobk` sent in the conversation.\n",
    ">* You can use the index (order) found in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Okay, `jacobk` has sent 31 emails in 13 different threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's do text mining:\n",
    ">* (1) Lowercasing\n",
    ">* (2) Tokenization\n",
    ">* (3) Removing stopwords\n",
    ">* (4) Removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* (1) Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * (2) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* (3) Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * (4) Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* If we did all the pre-processing steps correctly, we can find the most common words used by `jacobk` in the conversation.\n",
    ">* Q. What are the most 10 common words used by `jacobk`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's build edges from the `jacobkdf` dataframe.\n",
    ">* To do so, let's iterate over the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's get rid of the self-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's get rid of the duplicate edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* How many edges are there in the final graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

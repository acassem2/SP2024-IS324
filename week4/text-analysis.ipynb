{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Text analysis is an essential part of social network analysis.\n",
    "> * Imagine you have a social media dataset with tweets. You want to analyze text to find nodes and edges as well as what they are talking about.\n",
    "> * You can use text analysis to find the most frequent words, hashtags, and mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We are going to use the `nltk` library to analyze text.\n",
    "> * `from` is telling Python to import a library called `nltk`.\n",
    "> * `import` is used to import a specific module from a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('../week3/Political-media-DFE.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's import the data from week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
       "       '_last_judgment_at', 'audience', 'audience:confidence', 'bias',\n",
       "       'bias:confidence', 'message', 'message:confidence', 'orig__golden',\n",
       "       'audience_gold', 'bias_gold', 'bioid', 'embed', 'id', 'label',\n",
       "       'message_gold', 'source', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_unit_id                 int64\n",
       "_golden                   bool\n",
       "_unit_state             object\n",
       "_trusted_judgments       int64\n",
       "_last_judgment_at       object\n",
       "audience                object\n",
       "audience:confidence    float64\n",
       "bias                    object\n",
       "bias:confidence        float64\n",
       "message                 object\n",
       "message:confidence     float64\n",
       "orig__golden           float64\n",
       "audience_gold          float64\n",
       "bias_gold              float64\n",
       "bioid                   object\n",
       "embed                   object\n",
       "id                      object\n",
       "label                   object\n",
       "message_gold           float64\n",
       "source                  object\n",
       "text                    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `.dtypes` is used to check the data type of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's subset the data to have who posted, where they posted (social media platform), and what they posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=data[['label', 'source', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>I applaud Governor PerryÛªs recent decision t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>Today, I voted in favor of H.R. 5016 - Financi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>(Taken from posted WOKV interview)   Congressm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>Join me next week for a town hall in Ocala! I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>Foreign Affairs Committee Hearing on Syria. I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 label    source  \\\n",
       "0       From: Trey Radel (Representative from Florida)   twitter   \n",
       "1        From: Mitch McConnell (Senator from Kentucky)   twitter   \n",
       "2     From: Kurt Schrader (Representative from Oregon)   twitter   \n",
       "3             From: Michael Crapo (Senator from Idaho)   twitter   \n",
       "4             From: Mark Udall (Senator from Colorado)   twitter   \n",
       "...                                                ...       ...   \n",
       "4995      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4996      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4997      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4998      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4999      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "\n",
       "                                                   text  \n",
       "0     RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...  \n",
       "1     VIDEO - #Obamacare:  Full of Higher Costs and ...  \n",
       "2     Please join me today in remembering our fallen...  \n",
       "3     RT @SenatorLeahy: 1st step toward Senate debat...  \n",
       "4     .@amazon delivery #drones show need to update ...  \n",
       "...                                                 ...  \n",
       "4995  I applaud Governor PerryÛªs recent decision t...  \n",
       "4996  Today, I voted in favor of H.R. 5016 - Financi...  \n",
       "4997  (Taken from posted WOKV interview)   Congressm...  \n",
       "4998  Join me next week for a town hall in Ocala! I'...  \n",
       "4999  Foreign Affairs Committee Hearing on Syria. I ...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We have contents from both twitter and facebook and have a text column for the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['twitter', 'facebook'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['source'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's print out some of the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @nowthisnews: Rep. Trey Radel (R- #FL) slams #Obamacare. #politics https://t.co/zvywMG8yIH'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VIDEO - #Obamacare:  Full of Higher Costs and Broken Promises: http://t.co/dn3vzqIrWF'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please join me today in remembering our fallen heroes and honoring the men and women currently in military service for their sacrifices.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Q. What additional type of text do you see in the contents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```YOUR ANSWER HERE```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Yes, we see hashtags, retweets, mentions, and URLs.\n",
    "> * In the third row, we see an example of a content only with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * In text analysis, it is important to make the text clean (remove unnecessary words, symbols, etc.) and to make the text uniform (lowercase, no punctuation, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's lowercase all the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "content['text-lower']=content['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please join me today in remembering our fallen heroes and honoring the men and women currently in military service for their sacrifices.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We can seperate the entire contents into tokens (words, hashtags, mentions, etc.).\n",
    "> * Seperating the contents into tokens is called tokenization.\n",
    "> * We can use the `word_tokenize` function from the `nltk` library to tokenize the contents.\n",
    "> * There is also a `TweetTokenizer` function in the `nltk` library that is specifically for tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `.apply` is used to apply a function to a column. You don't have to use a for loop to apply a function to each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * There are two ways to tokenize the contents. One is to use `apply()` function to tokenize the lowercased text. \n",
    "> * `apply()` function allows you to apply a function along the axis of a DataFrame.\n",
    "> * Another way is to iterate through the lowercased text and tokenize each content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['tokenized_unigrams']=content['text-lower'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterated_unigrams=[]\n",
    "for idx, row in content.iterrows():\n",
    "    iterated_unigrams.append(word_tokenize(row['text-lower']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['iterated_unigrams']=iterated_unigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The results of iterating through each row and applying the `word_tokenize` function is a list of lists are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[0,'iterated_unigrams'] == content.loc[0,'tokenized_unigrams']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * One token is called a unigram.\n",
    "> * We can try to find bigrams (two tokens) and trigrams (three tokens) as well. \n",
    "> * All of these tokens are called n-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams lambda function\n",
    "content['tokenized_bigrams']=content['text-lower'].apply(TweetTokenizer().tokenize).apply(lambda x: list(ngrams(x, 2)))\n",
    "#It first applies the 'tokenize' method of the TweetTokenizer class to the 'text-lower' column,\n",
    "#and then applies the lambda function to the resulting list of bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Lambda function is created using the `lambda` keyword, followed by the input variable(s), a colon, and the function code.\n",
    "> * Lambda is a small, anonymous, and inline function.\n",
    "> * `lambda arguments : expression`\n",
    "> * e.g., `lambda x: x+1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#practicing lambda functions\n",
    "def square(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equivalent lambda function\n",
    "square_lambda = lambda x: x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_result = square(5)\n",
    "lambda_result = square_lambda(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(function_result)\n",
    "print(lambda_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The result of the lambda function and regular function is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram iteration\n",
    "iterated_bigrams=[]\n",
    "for idx, row in content.iterrows():\n",
    "    iterated_bigrams.append(list(ngrams(row['tokenized_unigrams'], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['iterated_bigrams']=iterated_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We are importing another library called `collections` to count the frequency of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's see how bigrams and unigrams are saved in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(rt, @nowthisnews), (@nowthisnews, :), (:, re...\n",
       "1       [(video, -), (-, #obamacare), (#obamacare, :),...\n",
       "2       [(please, join), (join, me), (me, today), (tod...\n",
       "3       [(rt, @senatorleahy), (@senatorleahy, :), (:, ...\n",
       "4       [(., @amazon), (@amazon, delivery), (delivery,...\n",
       "                              ...                        \n",
       "4995    [(i, applaud), (applaud, governor), (governor,...\n",
       "4996    [(today, ,), (,, i), (i, voted), (voted, in), ...\n",
       "4997    [((, taken), (taken, from), (from, posted), (p...\n",
       "4998    [(join, me), (me, next), (next, week), (week, ...\n",
       "4999    [(foreign, affairs), (affairs, committee), (co...\n",
       "Name: tokenized_bigrams, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['tokenized_bigrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [rt, @, nowthisnews, :, rep., trey, radel, (, ...\n",
       "1       [video, -, #, obamacare, :, full, of, higher, ...\n",
       "2       [please, join, me, today, in, remembering, our...\n",
       "3       [rt, @, senatorleahy, :, 1st, step, toward, se...\n",
       "4       [., @, amazon, delivery, #, drones, show, need...\n",
       "                              ...                        \n",
       "4995    [i, applaud, governor, perryûªs, recent, deci...\n",
       "4996    [today, ,, i, voted, in, favor, of, h.r, ., 50...\n",
       "4997    [(, taken, from, posted, wokv, interview, ), c...\n",
       "4998    [join, me, next, week, for, a, town, hall, in,...\n",
       "4999    [foreign, affairs, committee, hearing, on, syr...\n",
       "Name: tokenized_unigrams, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['tokenized_unigrams']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's count the most frequent unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7668),\n",
       " ('.', 6857),\n",
       " ('to', 5913),\n",
       " (',', 4927),\n",
       " (':', 3763),\n",
       " ('and', 3678),\n",
       " ('of', 3371),\n",
       " ('in', 2892),\n",
       " ('#', 2518),\n",
       " ('a', 2508)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['tokenized_unigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's count the most frequent bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 766),\n",
       " (('\\x89', 'ûªs'), 684),\n",
       " (('in', 'the'), 547),\n",
       " ((',', 'and'), 547),\n",
       " (('on', 'the'), 416),\n",
       " (('to', 'the'), 398),\n",
       " (('.', 'i'), 328),\n",
       " (('for', 'the'), 307),\n",
       " (('\\x89', 'û'), 305),\n",
       " (('at', 'the'), 305)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['tokenized_bigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * However, we see frequent words include function words (e.g., the, and, is, etc.) and punctuation.\n",
    "> * We can remove function words and punctuation to find the content words (e.g., nouns, verbs, adjectives, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>text-lower</th>\n",
       "      <th>tokenized_unigrams</th>\n",
       "      <th>iterated_unigrams</th>\n",
       "      <th>tokenized_bigrams</th>\n",
       "      <th>iterated_bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "      <td>rt @nowthisnews: rep. trey radel (r- #fl) slam...</td>\n",
       "      <td>[rt, @, nowthisnews, :, rep., trey, radel, (, ...</td>\n",
       "      <td>[rt, @, nowthisnews, :, rep., trey, radel, (, ...</td>\n",
       "      <td>[(rt, @nowthisnews), (@nowthisnews, :), (:, re...</td>\n",
       "      <td>[(rt, @), (@, nowthisnews), (nowthisnews, :), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "      <td>video - #obamacare:  full of higher costs and ...</td>\n",
       "      <td>[video, -, #, obamacare, :, full, of, higher, ...</td>\n",
       "      <td>[video, -, #, obamacare, :, full, of, higher, ...</td>\n",
       "      <td>[(video, -), (-, #obamacare), (#obamacare, :),...</td>\n",
       "      <td>[(video, -), (-, #), (#, obamacare), (obamacar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            label   source  \\\n",
       "0  From: Trey Radel (Representative from Florida)  twitter   \n",
       "1   From: Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "\n",
       "                                                text  \\\n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...   \n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...   \n",
       "\n",
       "                                          text-lower  \\\n",
       "0  rt @nowthisnews: rep. trey radel (r- #fl) slam...   \n",
       "1  video - #obamacare:  full of higher costs and ...   \n",
       "\n",
       "                                  tokenized_unigrams  \\\n",
       "0  [rt, @, nowthisnews, :, rep., trey, radel, (, ...   \n",
       "1  [video, -, #, obamacare, :, full, of, higher, ...   \n",
       "\n",
       "                                   iterated_unigrams  \\\n",
       "0  [rt, @, nowthisnews, :, rep., trey, radel, (, ...   \n",
       "1  [video, -, #, obamacare, :, full, of, higher, ...   \n",
       "\n",
       "                                   tokenized_bigrams  \\\n",
       "0  [(rt, @nowthisnews), (@nowthisnews, :), (:, re...   \n",
       "1  [(video, -), (-, #obamacare), (#obamacare, :),...   \n",
       "\n",
       "                                    iterated_bigrams  \n",
       "0  [(rt, @), (@, nowthisnews), (nowthisnews, :), ...  \n",
       "1  [(video, -), (-, #), (#, obamacare), (obamacar...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * In the `nltk` library, there is a list of stopwords (function words) that we can use to remove from the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop[1:10] #use slice to show only the first 10 stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop[-10:] #use negative index to slice the last 10 stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We want to remove the stopwords from the text-lower column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['stopword']=content['text-lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "#The lambda function takes each row of the 'text-lower' column, splits it into a list of words, \n",
    "#and then joins the words back together into a string, excluding any words that are in the 'stop' list.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's compare the result of removing stopwords and the original text-lower column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       rt @nowthisnews: rep. trey radel (r- #fl) slam...\n",
       "1       video - #obamacare: full higher costs broken p...\n",
       "2       please join today remembering fallen heroes ho...\n",
       "3       rt @senatorleahy: 1st step toward senate debat...\n",
       "4       .@amazon delivery #drones show need update law...\n",
       "                              ...                        \n",
       "4995    applaud governor perryûªs recent decision dep...\n",
       "4996    today, voted favor h.r. 5016 - financial servi...\n",
       "4997    (taken posted wokv interview) congressman yoho...\n",
       "4998    join next week town hall ocala! i'll answer qu...\n",
       "4999    foreign affairs committee hearing syria. remai...\n",
       "Name: stopword, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['stopword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       rt @nowthisnews: rep. trey radel (r- #fl) slam...\n",
       "1       video - #obamacare:  full of higher costs and ...\n",
       "2       please join me today in remembering our fallen...\n",
       "3       rt @senatorleahy: 1st step toward senate debat...\n",
       "4       .@amazon delivery #drones show need to update ...\n",
       "                              ...                        \n",
       "4995    i applaud governor perryûªs recent decision t...\n",
       "4996    today, i voted in favor of h.r. 5016 - financi...\n",
       "4997    (taken from posted wokv interview)   congressm...\n",
       "4998    join me next week for a town hall in ocala! i'...\n",
       "4999    foreign affairs committee hearing on syria. i ...\n",
       "Name: text-lower, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We'll get unigrams for the text after we remove function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['stop_tokenized_unigrams']=content['stopword'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We'll get bigrams for the text after we remove function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['stop_tokenized_bigrams']=content['stopword'].apply(TweetTokenizer().tokenize).apply(lambda x: list(ngrams(x, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Here are the most frequent unigrams and bigrams after removing function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 6863),\n",
       " (',', 4927),\n",
       " (':', 3763),\n",
       " ('#', 2518),\n",
       " ('http', 2162),\n",
       " ('@', 1877),\n",
       " ('!', 995),\n",
       " (\"'s\", 812),\n",
       " ('today', 784),\n",
       " ('&', 589)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['stop_tokenized_unigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('\\x89', 'ûªs'), 684),\n",
       " (('\\x89', 'û'), 305),\n",
       " (('û', '\\x9d'), 249),\n",
       " (('.', '\\x89'), 198),\n",
       " (('s', '.'), 193),\n",
       " (('u', '.'), 192),\n",
       " (('.', 's'), 190),\n",
       " (('here', ':'), 183),\n",
       " (('.', '\"'), 154),\n",
       " (('\\x89', 'ûò'), 149)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['stop_tokenized_bigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We still see irrelevant punctuations. Let's get rid of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['punct_tokenized_unigrams']=content['stop_tokenized_unigrams'].apply(lambda x: [word for word in x if word.isalnum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_iterated_bigrams=[]\n",
    "for idx, row in content.iterrows():\n",
    "    punct_iterated_bigrams.append(list(ngrams(row['punct_tokenized_unigrams'], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['punct_tokenized_bigrams']=punct_iterated_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's count the most frequent unigrams and bigrams after removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http', 2162),\n",
       " ('today', 784),\n",
       " ('house', 435),\n",
       " ('amp', 431),\n",
       " ('great', 396),\n",
       " ('new', 361),\n",
       " ('bill', 324),\n",
       " ('president', 317),\n",
       " ('act', 294),\n",
       " ('congress', 289)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['punct_tokenized_unigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('here', 'http'), 159),\n",
       " (('health', 'care'), 97),\n",
       " (('president', 'obama'), 83),\n",
       " (('united', 'states'), 75),\n",
       " (('town', 'hall'), 67),\n",
       " (('high', 'school'), 66),\n",
       " (('immigration', 'reform'), 61),\n",
       " (('small', 'business'), 47),\n",
       " (('last', 'night'), 46),\n",
       " (('make', 'sure'), 46)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['punct_tokenized_bigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * But we want to do additional cleaning. \n",
    "> * When counting the most frequent words, past and present tense of the same word are counted as different words.\n",
    "> * For example, \"run\" and \"running\" are counted as different words.\n",
    "> * We can use lemmatization to convert words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer() #Initialize lemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['lemma']=content['punct_tokenized_unigrams'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['lemma_str']=content['lemma'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's compare the results of lemmatization and without lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'called usdotfra release info inspection casseltonderailment review quality rail'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[5, 'lemma_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'called on the @usdotfra to release info about inspections before the #casseltonderailment to review quality of rails. (1/2)'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[5, 'text-lower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Q. Do you see any different results? What tokens have changed after lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The punctuation, symbols, and stopwords have been removed so that the lemmatized string is limited to relevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better', pos=wordnet.ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cars'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('cars', pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Interestingly enough, NLTK's WordNetLemmatizer is not perfect.\n",
    "> * By default, it only lemmatize nouns.\n",
    "> * Therefore, we need to specify the part of speech (POS) for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'): #ADJECTIVE\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'): #VERN\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'): #NOUN        \n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'): #ADVERB\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    # Tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged) \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # If no tag was found, then use the word as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            # Else use the tag to lemmatize the word\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['lemmatizer_str']=content['lemma'].apply(lambda x: lemmatize_sentence(' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'called usdotfra release info inspection casseltonderailment review quality rail'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[5, 'lemma_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call usdotfra release info inspection casseltonderailment review quality rail'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[5, 'lemmatizer_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's extract hashtags and mentions from the contents.\n",
    "> * We have to use regular expressions to extract hashtags and mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * There are three useful regex functions in Python: `findall`, `search`, and `match`.\n",
    "\n",
    "> * `findall` is used to find all matches of a pattern in a string.\n",
    "> * `search` is used to find the first match of a pattern in a string.\n",
    "> * `match` is used to match a pattern at the beginning of a string.\n",
    "\n",
    "> * In this class, we are mostly going to use `findall` to extract hashtags and mentions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The result of `findall` is a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Basic regex patterns:\n",
    "> * `.` matches any character except a newline.\n",
    "> * `*` matches 0 or more repetitions of the preceding regex pattern.\n",
    "> * `+` matches 1 or more repetitions of the preceding regex pattern.\n",
    "> * `?` matches 0 or 1 repetition of the preceding regex pattern.\n",
    "> * `^` matches the start of a string.\n",
    "> * `$` matches the end of a string.\n",
    "> * `[]` matches any one of the characters inside the square brackets.\n",
    "> * `\\` is used to escape special characters.\n",
    "> * `|` is used to match either the regex pattern on the left or the right.\n",
    "\n",
    "> * `[a-z]` matches any lowercase letter.\n",
    "> * `[A-Z]` matches any uppercase letter.\n",
    "> * `[0-9]` matches any digit.\n",
    "> * `\\d` matches any digit.\n",
    "> * `\\D` matches any non-digit.\n",
    "> * `\\w` matches any word character (alphanumeric and underscore).\n",
    "> * `\\W` matches any non-word character.\n",
    "> * `\\s` matches any whitespace character.\n",
    "> * `\\S` matches any non-whitespace character.\n",
    "\n",
    "> * `[a-zA-Z]` matches any alphabet character.\n",
    "> * `[a-zA-Z0-9]` matches any alphanumeric character.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../week4/Regex-Cheat-Sheet.png\" width=500px height=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's practice regex with the `findall` function.\n",
    "> * Return all non-overlapping matches of pattern in string, as a list of strongs or tuples. The string is scanned left-to-right, and matches are returned in the order found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '3']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'\\d+') # \\d+ is the pattern to search for, it means any digit 0-9 and the + means one or more times\n",
    "sample1='I have 2 dogs and 3 cats'\n",
    "result=re.findall(pattern, sample1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is324socialnetworkanalysis@illinois.edu', 'jaihyunpark@illinois.edu']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}') \n",
    "#[a-zA-Z0-9._%+-]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the characters ._%+-, and the + means one or more times\n",
    "#@ is the pattern to search for, it means the character @\n",
    "#[a-zA-Z0-9.-]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the characters .-, and the + means one or more times\n",
    "#\\ is the escape character, it is used to escape the . character, so it is not interpreted as a special character\n",
    "#[a-zA-Z]{2,} is the pattern to search for, it means any letter a-z or A-Z, and the {2,} means two or more times\n",
    "sample2='''My primary email is is324socialnetworkanalysis@illinois.edu. Please contact me! \n",
    "If I am not replying, you can contact me at jaihyunpark@illinois.edu'''\n",
    "result=re.findall(pattern, sample2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://is324.com']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'https?://\\S+')\n",
    "#http is the pattern to search for, it means the characters http\n",
    "#s? is the pattern to search for, it means the character s, and the ? means zero or one time\n",
    "#:// is the pattern to search for, it means the characters ://\n",
    "#\\S+ is the pattern to search for, it means any character that is not a white space, and the + means one or more times\n",
    "sample3='Look at what is happening in Washington! https://is324.com'\n",
    "result=re.findall(pattern, sample3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#IS324']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'#[a-zA-Z0-9]+')\n",
    "## is the pattern to search for, it means the character #\n",
    "#[a-zA-Z0-9]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the + means one or more times\n",
    "sample4='I am so excited for the class #IS324'\n",
    "result=re.findall(pattern, sample4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@jaihyunpark']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'@[a-zA-Z0-9]+')\n",
    "#@ is the pattern to search for, it means the character @\n",
    "#[a-zA-Z0-9]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the + means one or more times\n",
    "sample5='@jaihyunpark is the instructor for #IS324'\n",
    "result=re.findall(pattern, sample5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' @jaihyunpark', ' @socialmedia']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'(rt\\s+@[a-zA-Z0-9]+ | @[a-zA-Z0-9]+)')\n",
    "#The pattern is composed of two subpatterns, separated by the | character, \n",
    "#which means \"or\" and grouped by parentheses.\n",
    "#rt is the pattern to search for, it means the characters rt\n",
    "#\\s+ is the pattern to search for, it means any white space, and the + means one or more times\n",
    "#@ is the pattern to search for, it means the character @\n",
    "#[a-zA-Z0-9]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the + means one or more times\n",
    "\n",
    "#Another subpattern is looking for mentions without the rt prefix\n",
    "#@ is the pattern to search for, it means the character @\n",
    "#[a-zA-Z0-9]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the + means one or more times\n",
    "sample6='rt @jaihyunpark: I am so excited for the class #IS324 @socialmedia'\n",
    "result=re.findall(pattern, sample6)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's practice regex with the `search` function.\n",
    "> * Scan through the string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(7, 8), match='2'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'\\d+') # \\d+ is the pattern to search for, it means \n",
    "#any digit 0-9 and the + means one or more times\n",
    "sample1='I have 2 dogs and 3 cats'\n",
    "result=re.search(pattern, sample1) #search method returns the index of the first match\n",
    "print(result)\n",
    "# span=(7, 8) means that the match was found from the 7th to the 8th character of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(sample1[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(20, 59), match='is324socialnetworkanalysis@illinois.edu'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}') \n",
    "#Matches one or more alphanumeric characters, dots, underscores, percent signs, plus signs, or hyphens. \n",
    "#The @ symbol is also matched.\n",
    "sample2='''My primary email is is324socialnetworkanalysis@illinois.edu. \n",
    "Please contact me! If I am not replying, you can contact me at jaihyunpark@illinois.edu'''\n",
    "result=re.search(pattern, sample2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is324socialnetworkanalysis@illinois.edu\n"
     ]
    }
   ],
   "source": [
    "print(sample2[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(41, 58), match='https://is324.com'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'https?://\\S+')\n",
    "#Matches http or https, followed by ://, followed by any non-whitespace characters.\n",
    "sample3='Look at what is happening in Washington! https://is324.com'\n",
    "result=re.search(pattern, sample3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://is324.com\n"
     ]
    }
   ],
   "source": [
    "print(sample3[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(30, 36), match='#IS324'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'#[a-zA-Z0-9]+')\n",
    "#Matches hashtags that start with #, followed by one or more alphanumeric characters.\n",
    "sample4='I am so excited for the class #IS324'\n",
    "result=re.search(pattern, sample4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#IS324\n"
     ]
    }
   ],
   "source": [
    "print(sample4[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 12), match='@jaihyunpark'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'@[a-zA-Z0-9]+')\n",
    "#Matches mentions that start with @, followed by one or more alphanumeric characters.\n",
    "sample5='@jaihyunpark is the instructor for #IS324'\n",
    "result=re.search(pattern, sample5) #search method returns the index of the first match\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@jaihyunpark\n"
     ]
    }
   ],
   "source": [
    "print(sample5[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(2, 15), match=' @jaihyunpark'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'(rt\\s+@[a-zA-Z0-9]+ | @[a-zA-Z0-9]+)')\n",
    "#Matches retweets that start with rt, followed by one or more white spaces, \n",
    "#followed by @, followed by one or more alphanumeric characters.\n",
    "sample6='rt @jaihyunpark: I am so excited for the class #IS324 @socialmedia'\n",
    "result=re.search(pattern, sample6)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @jaihyunpark\n"
     ]
    }
   ],
   "source": [
    "print(sample6[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's practice regex with `match` function.\n",
    "> * If zero or more characters at the beginning of string match the regular expression pattern, return a corresponding match object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'\\d+') # \\d+ is the pattern to search for, \n",
    "#it means any digit 0-9 and the + means one or more times\n",
    "sample1='I have 2 dogs and 3 cats'\n",
    "result=re.match(pattern, sample1) #check if the pattern is at the beginning of the string\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's extract hashtags and mentions from the content DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * First, we are extracting mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'@[a-zA-Z0-9]+')\n",
    "content['mentions']=content['text-lower'].apply(lambda x: re.findall(pattern, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's check if mention is extracted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @nowthisnews: rep. trey radel (r- #fl) slams #obamacare. #politics https://t.co/zvywmg8yih'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@nowthisnews']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['mentions'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'video - #obamacare:  full of higher costs and broken promises: http://t.co/dn3vzqirwf'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['mentions'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's extract hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'#([a-zA-Z0-9]+)')\n",
    "content['hashtags']=content['text-lower'].apply(lambda x: re.findall(pattern, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @nowthisnews: rep. trey radel (r- #fl) slams #obamacare. #politics https://t.co/zvywmg8yih'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fl', 'obamacare', 'politics']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['hashtags'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'video - #obamacare:  full of higher costs and broken promises: http://t.co/dn3vzqirwf'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obamacare']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['hashtags'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We can further extract https links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=re.compile(r'https?://\\S+')\n",
    "content['http']=content['text-lower'].apply(lambda x: re.findall(pattern, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @nowthisnews: rep. trey radel (r- #fl) slams #obamacare. #politics https://t.co/zvywmg8yih'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://t.co/zvywmg8yih']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['http'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Pandas Series also has a `str` attribute to apply string methods.\n",
    ">* The useful built-in functions are `str.contains`,  `str.replace`, and `str.findall`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `str.contains` is used to check if a pattern is contained in each string of the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        True\n",
       "1        True\n",
       "2       False\n",
       "3       False\n",
       "4        True\n",
       "        ...  \n",
       "4995    False\n",
       "4996    False\n",
       "4997     True\n",
       "4998    False\n",
       "4999    False\n",
       "Name: text-lower, Length: 5000, dtype: bool"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].str.contains('http')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * How many tweets contain a URL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2171, 19)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[content['text-lower'].str.contains('http')].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* `str.replace` is used to replace a pattern with another pattern in each string of the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['no-url']=content['text-lower'].str.replace(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @nowthisnews: rep. trey radel (r- #fl) slams #obamacare. #politics https://t.co/zvywmg8yih'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @nowthisnews: rep. trey radel (r- #fl) slams #obamacare. #politics '"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['no-url'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* `str.findall` is used to find all matches of a pattern in each string of the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['mentions-str']=content['text-lower'].str.findall(r'@([a-zA-Z0-9]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The result of `str.findall` in Pandas is identical to the regular expression `findall` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['huckabeeshow',\n",
       " 'foxnews',\n",
       " 'huckabeeshow',\n",
       " 'grahamblog',\n",
       " 'replankford',\n",
       " 'kimguilfoyle',\n",
       " 'markgeragos',\n",
       " 'gopblackchick',\n",
       " 'rcamposduffy']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['mentions-str'].iloc[2277]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@huckabeeshow',\n",
       " '@foxnews',\n",
       " '@huckabeeshow',\n",
       " '@grahamblog',\n",
       " '@replankford',\n",
       " '@kimguilfoyle',\n",
       " '@markgeragos',\n",
       " '@gopblackchick',\n",
       " '@rcamposduffy']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['mentions'].iloc[2277]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's rename the column name for 'label' into 'from'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "content.rename(columns={\"label\" : \"from\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We created the column that contains the mentions in 'mentions' column. How many unique accounts have been mentioned?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * You might want to flatten the list of mentions and then count the frequency of each mention.\n",
    "> * This code below will flatten the list of mentions.\n",
    "> * `[item for sublist in content['mentions'] for item in sublist]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@speakerboehner    17\n",
       "@washingtonpost    13\n",
       "@whitehouse        11\n",
       "@foxnews           11\n",
       "@usatoday           9\n",
       "@wsj                9\n",
       "@barackobama        9\n",
       "@msnbc              8\n",
       "@deptvetaffairs     8\n",
       "@thehill            8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[item for sublist in content['mentions'] for item in sublist]\n",
    "pd.Series([item \n",
    "           for sublist in content['mentions'] \n",
    "            for item in sublist]).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We created the column that contains hashtags in 'hashtags' column. How many unique hashtags have been used?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * You might want to flatten the list of hashtags.\n",
    "> * This code below will flatten the list of hashtags.\n",
    "> * `[item for sublist in content['hashtags'] for item in sublist]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obamacare       87\n",
       "irs             28\n",
       "renewui         26\n",
       "jobs            22\n",
       "raisethewage    22\n",
       "tbt             21\n",
       "sotu            21\n",
       "benghazi        20\n",
       "veterans        19\n",
       "tcot            17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for sublist in content['hashtags'] for item in sublist]\n",
    "pd.Series([item for sublist in content['hashtags'] for item in sublist]).value_counts().head(10)\n",
    "#first part flattens the list, then the value coutns are found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Who mentioned the most in one tweet?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: James Lankford (Representative from Oklahoma)'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mentions = [[content[\"mentions-str\"]]]\n",
    "#mentions\n",
    "\n",
    "content['from'].iloc[content['mentions'].apply(lambda x: len(x)).sort_values(ascending=False).index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks like `YOUR ANSWER` mentioned nine accounts in one tweet. What is the content of the tweet? Use 'text-lower' column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mt @huckabeeshow sat @foxnews @huckabeeshow guests: @grahamblog @replankford @kimguilfoyle @markgeragos @gopblackchick @rcamposduffy'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].iloc[content['mentions'].apply(lambda x: len(x)).sort_values(ascending=False).index[0]]\n",
    "#iloc you put the index so that you can access the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Who used hashtags the most in one tweet?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: Dina Titus (Representative from Nevada)'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['from'].iloc[content['hashtags'].apply(lambda x: len(x)).sort_values(ascending=False).index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks like `YOUR ANSWER` used eight hastags in one tweet. What is the content of the tweet that used the most hashtags? Use 'text-lower' column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'always enjoy mtg w/ #leaders of #lvbiz community @lvchamber #eggsandissues #transportation #uav #i11 #biz #jobs http://t.co/nlqj7oq2w0'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].iloc[content['hashtags'].apply(lambda x: len(x)).sort_values(ascending=False).index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the 'from' column, the name of the politician comes with 'From: ' in front of the name and whether the politician is Representative or Senator. Let's clean this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, clean the 'From: ' in the 'from' column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Trey Radel (Representative from Florida)\n",
       "1          Mitch McConnell (Senator from Kentucky)\n",
       "2       Kurt Schrader (Representative from Oregon)\n",
       "3               Michael Crapo (Senator from Idaho)\n",
       "4               Mark Udall (Senator from Colorado)\n",
       "                           ...                    \n",
       "4995        Ted Yoho (Representative from Florida)\n",
       "4996        Ted Yoho (Representative from Florida)\n",
       "4997        Ted Yoho (Representative from Florida)\n",
       "4998        Ted Yoho (Representative from Florida)\n",
       "4999        Ted Yoho (Representative from Florida)\n",
       "Name: from, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['from']=content['from'].str.replace('From: ', '')\n",
    "content['from']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second, extract where they are from (the name of the State) and create a new column called 'state'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=re.compile(r'(.+?)\\s*\\(')\n",
    "content['politician']=content['from'].str.findall(pattern)\n",
    "#Consistently, name of the state starts with \"From\"\n",
    "#Find string that starts with From, needs a space, followed by anything uppercase or lowercase, + means more than 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third, extract the name of the politician and create a new column called 'politician'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [Trey Radel]\n",
       "1       [Mitch McConnell]\n",
       "2         [Kurt Schrader]\n",
       "3         [Michael Crapo]\n",
       "4            [Mark Udall]\n",
       "              ...        \n",
       "4995           [Ted Yoho]\n",
       "4996           [Ted Yoho]\n",
       "4997           [Ted Yoho]\n",
       "4998           [Ted Yoho]\n",
       "4999           [Ted Yoho]\n",
       "Name: politician, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['politician']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

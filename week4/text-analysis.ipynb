{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Text analysis is an essential part of social network analysis.\n",
    "> * Imagine you have a social media dataset with tweets. You want to analyze text to find nodes and edges as well as what they are talking about.\n",
    "> * You can use text analysis to find the most frequent words, hashtags, and mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We are going to use the `nltk` library to analyze text.\n",
    "> * `from` is telling Python to import a library called `nltk`.\n",
    "> * `import` is used to import a specific module from a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('../week3/Political-media-DFE.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's import the data from week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `.dtypes` is used to check the data type of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's subset the data to have who posted, where they posted (social media platform), and what they posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=data[['label', 'source', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We have contents from both twitter and facebook and have a text column for the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['source'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's print out some of the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Q. What additional type of text do you see in the contents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```YOUR ANSWER HERE```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Yes, we see hashtags, retweets, mentions, and URLs.\n",
    "> * In the third row, we see an example of a content only with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * In text analysis, it is important to make the text clean (remove unnecessary words, symbols, etc.) and to make the text uniform (lowercase, no punctuation, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's lowercase all the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text-lower']=content['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text-lower'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We can seperate the entire contents into tokens (words, hashtags, mentions, etc.).\n",
    "> * Seperating the contents into tokens is called tokenization.\n",
    "> * We can use the `word_tokenize` function from the `nltk` library to tokenize the contents.\n",
    "> * There is also a `TweetTokenizer` function in the `nltk` library that is specifically for tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `.apply` is used to apply a function to a column. You don't have to use a for loop to apply a function to each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * There are two ways to tokenize the contents. One is to use `apply()` function to tokenize the lowercased text. \n",
    "> * `apply()` function allows you to apply a function along the axis of a DataFrame.\n",
    "> * Another way is to iterate through the lowercased text and tokenize each content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['tokenized_unigrams']=content['text-lower'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterated_unigrams=[]\n",
    "for idx, row in content.iterrows():\n",
    "    iterated_unigrams.append(word_tokenize(row['text-lower']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['iterated_unigrams']=iterated_unigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The results of iterating through each row and applying the `word_tokenize` function is a list of lists are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.loc[0,'iterated_unigrams'] == content.loc[0,'tokenized_unigrams']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * One token is called a unigram.\n",
    "> * We can try to find bigrams (two tokens) and trigrams (three tokens) as well. \n",
    "> * All of these tokens are called n-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams lambda function\n",
    "content['tokenized_bigrams']=content['text-lower'].apply(TweetTokenizer().tokenize).apply(lambda x: list(ngrams(x, 2)))\n",
    "#It first applies the 'tokenize' method of the TweetTokenizer class to the 'text-lower' column,\n",
    "#and then applies the lambda function to the resulting list of bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Lambda function is created using the `lambda` keyword, followed by the input variable(s), a colon, and the function code.\n",
    "> * Lambda is a small, anonymous, and inline function.\n",
    "> * `lambda arguments : expression`\n",
    "> * e.g., `lambda x: x+1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#practicing lambda functions\n",
    "def square(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equivalent lambda function\n",
    "square_lambda = lambda x: x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_result = square(5)\n",
    "lambda_result = square_lambda(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(function_result)\n",
    "print(lambda_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The result of the lambda function and regular function is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram iteration\n",
    "iterated_bigrams=[]\n",
    "for idx, row in content.iterrows():\n",
    "    iterated_bigrams.append(list(ngrams(row['tokenized_unigrams'], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['iterated_bigrams']=iterated_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We are importing another library called `collections` to count the frequency of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's see how bigrams and unigrams are saved in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['tokenized_bigrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['tokenized_unigrams']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's count the most frequent unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([item for row in content['tokenized_unigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's count the most frequent bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([item for row in content['tokenized_bigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * However, we see frequent words include function words (e.g., the, and, is, etc.) and punctuation.\n",
    "> * We can remove function words and punctuation to find the content words (e.g., nouns, verbs, adjectives, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * In the `nltk` library, there is a list of stopwords (function words) that we can use to remove from the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop[1:10] #use slice to show only the first 10 stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop[-10:] #use negative index to slice the last 10 stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We want to remove the stopwords from the text-lower column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['stopword']=content['text-lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "#The lambda function takes each row of the 'text-lower' column, splits it into a list of words, \n",
    "#and then joins the words back together into a string, excluding any words that are in the 'stop' list.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's compare the result of removing stopwords and the original text-lower column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['stopword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text-lower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We'll get unigrams for the text after we remove function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['stop_tokenized_unigrams']=content['stopword'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We'll get bigrams for the text after we remove function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['stop_tokenized_bigrams']=content['stopword'].apply(TweetTokenizer().tokenize).apply(lambda x: list(ngrams(x, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Here are the most frequent unigrams and bigrams after removing function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([item for row in content['stop_tokenized_unigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([item for row in content['stop_tokenized_bigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We still see irrelevant punctuations. Let's get rid of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['punct_tokenized_unigrams']=content['stop_tokenized_unigrams'].apply(lambda x: [word for word in x if word.isalnum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_iterated_bigrams=[]\n",
    "for idx, row in content.iterrows():\n",
    "    punct_iterated_bigrams.append(list(ngrams(row['punct_tokenized_unigrams'], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['punct_tokenized_bigrams']=punct_iterated_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's count the most frequent unigrams and bigrams after removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([item for row in content['punct_tokenized_unigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([item for row in content['punct_tokenized_bigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * But we want to do additional cleaning. \n",
    "> * When counting the most frequent words, past and present tense of the same word are counted as different words.\n",
    "> * For example, \"run\" and \"running\" are counted as different words.\n",
    "> * We can use lemmatization to convert words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer() #Initialize lemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['lemma']=content['punct_tokenized_unigrams'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['lemma_str']=content['lemma'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's compare the results of lemmatization and without lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.loc[5, 'lemma_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.loc[5, 'text-lower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Q. Do you see any different results? What tokens have changed after lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YOUR ANSWER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('better', pos=wordnet.ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('cars', pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Interestingly enough, NLTK's WordNetLemmatizer is not perfect.\n",
    "> * By default, it only lemmatize nouns.\n",
    "> * Therefore, we need to specify the part of speech (POS) for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'): #ADJECTIVE\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'): #VERN\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'): #NOUN        \n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'): #ADVERB\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    # Tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged) \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # If no tag was found, then use the word as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            # Else use the tag to lemmatize the word\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['lemmatizer_str']=content['lemma'].apply(lambda x: lemmatize_sentence(' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.loc[5, 'lemma_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.loc[5, 'lemmatizer_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's extract hashtags and mentions from the contents.\n",
    "> * We have to use regular expressions to extract hashtags and mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * There are three useful regex functions in Python: `findall`, `search`, and `match`.\n",
    "\n",
    "> * `findall` is used to find all matches of a pattern in a string.\n",
    "> * `search` is used to find the first match of a pattern in a string.\n",
    "> * `match` is used to match a pattern at the beginning of a string.\n",
    "\n",
    "> * In this class, we are mostly going to use `findall` to extract hashtags and mentions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The result of `findall` is a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Basic regex patterns:\n",
    "> * `.` matches any character except a newline.\n",
    "> * `*` matches 0 or more repetitions of the preceding regex pattern.\n",
    "> * `+` matches 1 or more repetitions of the preceding regex pattern.\n",
    "> * `?` matches 0 or 1 repetition of the preceding regex pattern.\n",
    "> * `^` matches the start of a string.\n",
    "> * `$` matches the end of a string.\n",
    "> * `[]` matches any one of the characters inside the square brackets.\n",
    "> * `\\` is used to escape special characters.\n",
    "> * `|` is used to match either the regex pattern on the left or the right.\n",
    "\n",
    "> * `[a-z]` matches any lowercase letter.\n",
    "> * `[A-Z]` matches any uppercase letter.\n",
    "> * `[0-9]` matches any digit.\n",
    "> * `\\d` matches any digit.\n",
    "> * `\\D` matches any non-digit.\n",
    "> * `\\w` matches any word character (alphanumeric and underscore).\n",
    "> * `\\W` matches any non-word character.\n",
    "> * `\\s` matches any whitespace character.\n",
    "> * `\\S` matches any non-whitespace character.\n",
    "\n",
    "> * `[a-zA-Z]` matches any alphabet character.\n",
    "> * `[a-zA-Z0-9]` matches any alphanumeric character.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../week4/Regex-Cheat-Sheet.png\" width=500px height=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's practice regex with the `findall` function.\n",
    "> * Return all non-overlapping matches of pattern in string, as a list of strongs or tuples. The string is scanned left-to-right, and matches are returned in the order found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\d+') # \\d+ is the pattern to search for, it means any digit 0-9 and the + means one or more times\n",
    "sample1='I have 2 dogs and 3 cats'\n",
    "result=re.findall(pattern, sample1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}') \n",
    "#[a-zA-Z0-9._%+-]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the characters ._%+-, and the + means one or more times\n",
    "#@ is the pattern to search for, it means the character @\n",
    "#[a-zA-Z0-9.-]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the characters .-, and the + means one or more times\n",
    "#\\ is the escape character, it is used to escape the . character, so it is not interpreted as a special character\n",
    "#[a-zA-Z]{2,} is the pattern to search for, it means any letter a-z or A-Z, and the {2,} means two or more times\n",
    "sample2='''My primary email is is324socialnetworkanalysis@illinois.edu. Please contact me! \n",
    "If I am not replying, you can contact me at jaihyunpark@illinois.edu'''\n",
    "result=re.findall(pattern, sample2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'https?://\\S+')\n",
    "#http is the pattern to search for, it means the characters http\n",
    "#s? is the pattern to search for, it means the character s, and the ? means zero or one time\n",
    "#:// is the pattern to search for, it means the characters ://\n",
    "#\\S+ is the pattern to search for, it means any character that is not a white space, and the + means one or more times\n",
    "sample3='Look at what is happening in Washington! https://is324.com'\n",
    "result=re.findall(pattern, sample3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'#[a-zA-Z0-9]+')\n",
    "## is the pattern to search for, it means the character #\n",
    "#[a-zA-Z0-9]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the + means one or more times\n",
    "sample4='I am so excited for the class #IS324'\n",
    "result=re.findall(pattern, sample4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'@[a-zA-Z0-9]+')\n",
    "#@ is the pattern to search for, it means the character @\n",
    "#[a-zA-Z0-9]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the + means one or more times\n",
    "sample5='@jaihyunpark is the instructor for #IS324'\n",
    "result=re.findall(pattern, sample5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'(rt\\s+@[a-zA-Z0-9]+ | @[a-zA-Z0-9]+)')\n",
    "#The pattern is composed of two subpatterns, separated by the | character, \n",
    "#which means \"or\" and grouped by parentheses.\n",
    "#rt is the pattern to search for, it means the characters rt\n",
    "#\\s+ is the pattern to search for, it means any white space, and the + means one or more times\n",
    "#@ is the pattern to search for, it means the character @\n",
    "#[a-zA-Z0-9]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the + means one or more times\n",
    "\n",
    "#Another subpattern is looking for mentions without the rt prefix\n",
    "#@ is the pattern to search for, it means the character @\n",
    "#[a-zA-Z0-9]+ is the pattern to search for, it means any letter a-z or A-Z, \n",
    "#any digit 0-9, and the + means one or more times\n",
    "sample6='rt @jaihyunpark: I am so excited for the class #IS324 @socialmedia'\n",
    "result=re.findall(pattern, sample6)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's practice regex with the `search` function.\n",
    "> * Scan through the string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\d+') # \\d+ is the pattern to search for, it means \n",
    "#any digit 0-9 and the + means one or more times\n",
    "sample1='I have 2 dogs and 3 cats'\n",
    "result=re.search(pattern, sample1) #search method returns the index of the first match\n",
    "print(result)\n",
    "# span=(7, 8) means that the match was found from the 7th to the 8th character of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample1[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}') \n",
    "#Matches one or more alphanumeric characters, dots, underscores, percent signs, plus signs, or hyphens. \n",
    "#The @ symbol is also matched.\n",
    "sample2='''My primary email is is324socialnetworkanalysis@illinois.edu. \n",
    "Please contact me! If I am not replying, you can contact me at jaihyunpark@illinois.edu'''\n",
    "result=re.search(pattern, sample2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample2[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'https?://\\S+')\n",
    "#Matches http or https, followed by ://, followed by any non-whitespace characters.\n",
    "sample3='Look at what is happening in Washington! https://is324.com'\n",
    "result=re.search(pattern, sample3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample3[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'#[a-zA-Z0-9]+')\n",
    "#Matches hashtags that start with #, followed by one or more alphanumeric characters.\n",
    "sample4='I am so excited for the class #IS324'\n",
    "result=re.search(pattern, sample4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample4[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'@[a-zA-Z0-9]+')\n",
    "#Matches mentions that start with @, followed by one or more alphanumeric characters.\n",
    "sample5='@jaihyunpark is the instructor for #IS324'\n",
    "result=re.search(pattern, sample5) #search method returns the index of the first match\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample5[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'(rt\\s+@[a-zA-Z0-9]+ | @[a-zA-Z0-9]+)')\n",
    "#Matches retweets that start with rt, followed by one or more white spaces, \n",
    "#followed by @, followed by one or more alphanumeric characters.\n",
    "sample6='rt @jaihyunpark: I am so excited for the class #IS324 @socialmedia'\n",
    "result=re.search(pattern, sample6)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample6[result.span()[0]:result.span()[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's practice regex with `match` function.\n",
    "> * If zero or more characters at the beginning of string match the regular expression pattern, return a corresponding match object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\d+') # \\d+ is the pattern to search for, \n",
    "#it means any digit 0-9 and the + means one or more times\n",
    "sample1='I have 2 dogs and 3 cats'\n",
    "result=re.match(pattern, sample1) #check if the pattern is at the beginning of the string\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's extract hashtags and mentions from the content DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * First, we are extracting mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'@[a-zA-Z0-9]+')\n",
    "content['mentions']=content['text-lower'].apply(lambda x: re.findall(pattern, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's check if mention is extracted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text-lower'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['mentions'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text-lower'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['mentions'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's extract hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'#([a-zA-Z0-9]+)')\n",
    "content['hashtags']=content['text-lower'].apply(lambda x: re.findall(pattern, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text-lower'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['hashtags'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text-lower'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['hashtags'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We can further extract https links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=re.compile(r'https?://\\S+')\n",
    "content['http']=content['text-lower'].apply(lambda x: re.findall(pattern, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text-lower'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['http'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Pandas Series also has a `str` attribute to apply string methods.\n",
    ">* The useful built-in functions are `str.contains`,  `str.replace`, and `str.findall`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `str.contains` is used to check if a pattern is contained in each string of the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text-lower'].str.contains('http')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * How many tweets contain a URL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content[content['text-lower'].str.contains('http')].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* `str.replace` is used to replace a pattern with another pattern in each string of the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['no-url']=content['text-lower'].str.replace(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['text-lower'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['no-url'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* `str.findall` is used to find all matches of a pattern in each string of the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['mentions-str']=content['text-lower'].str.findall(r'@([a-zA-Z0-9]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The result of `str.findall` in Pandas is identical to the regular expression `findall` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['mentions-str'].iloc[2277]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['mentions'].iloc[2277]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's rename the column name for 'label' into 'from'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We created the column that contains the mentions in 'mentions' column. How many unique accounts have been mentioned?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * You might want to flatten the list of mentions and then count the frequency of each mention.\n",
    "> * This code below will flatten the list of mentions.\n",
    "> * `[item for sublist in content['mentions'] for item in sublist]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We created the column that contains hashtags in 'hashtags' column. How many unique hashtags have been used?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * You might want to flatten the list of hashtags.\n",
    "> * This code below will flatten the list of hashtags.\n",
    "> * `[item for sublist in content['hashtags'] for item in sublist]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Who mentioned the most in one tweet?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks like `YOUR ANSWER` mentioned nine accounts in one tweet. What is the content of the tweet? Use 'text-lower' column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Who used hashtags the most in one tweet?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks like `YOUR ANSWER` used eight hastags in one tweet. What is the content of the tweet that used the most hashtags? Use 'text-lower' column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the 'from' column, the name of the politician comes with 'From: ' in front of the name and whether the politician is Representative or Senator. Let's clean this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, clean the 'From: ' in the 'from' column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second, extract where they are from (the name of the State) and create a new column called 'state'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third, extract the name of the politician and create a new column called 'politician'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
